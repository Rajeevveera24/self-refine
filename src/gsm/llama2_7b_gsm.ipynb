{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaModel, LlamaTokenizer, AutoModelForMaskedLM, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def load_model(model_name, token):\n",
    "    # Load model and tokenizer with authentication token\n",
    "    # model = LlamaModel.from_pretrained(model_name, use_auth_token=token)\n",
    "    # tokenizer = LlamaTokenizer.from_pretrained(model_name, use_auth_token=token)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token, cache_dir=\"/home/raj/models/\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=token, cache_dir=\"/home/raj/models/\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_model_from_local(model_file_path):\n",
    "    # Load model and tokenizer from local cache\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_file_path, local_files_only=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_file_path, local_files_only=True)\n",
    "    model = None\n",
    "    return model, tokenizer\n",
    "\n",
    "# Specify the model name and your Hugging Face access token\n",
    "model_name = \"reciprocate/llama2-7b-gsm8k\"\n",
    "your_token = \"\"  # Replace with your actual token\n",
    "model_file_path = \"/home/raj/.cache/huggingface/hub/models--reciprocate--llama2-7b-gsm8k/snapshots/a99b9c5a7e7b6c37dc6fd81cbc4fd2f2015b2967\"\n",
    "model, tokenizer = load_model(model_name, your_token)\n",
    "# model, tokenizer = load_model_from_local(model_file_path)\n",
    "\n",
    "# You can now continue with the rest of your inference code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, model, tokenizer):\n",
    "    # Encode the prompt to tokens\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    # Generate tokens response from the model\n",
    "    output_ids = model.generate(input_ids, max_length=100, num_return_sequences=2)\n",
    "    \n",
    "    print(output_ids.shape)    \n",
    "    # print(output_ids)\n",
    "    \n",
    "    # Decode the generated ids to a text string\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"3 + 5 = ?\"\n",
    "\n",
    "# Generate text based on the prompt\n",
    "generated_text = generate_text(prompt, model, tokenizer)\n",
    "print(\"Generated text:\", generated_text)\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "#CLear cude cache\n",
    "import torch, gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "pipe = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "pipe = pipeline(\"question-answering\", model=\"reciprocate/llama2-7b-gsm8k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Copy of code in utils.py\n",
    "\n",
    "import traceback\n",
    "\n",
    "class Prompt:\n",
    "    def __init__(\n",
    "        self,\n",
    "        question_prefix: str,\n",
    "        answer_prefix: str,\n",
    "        intra_example_sep: str,\n",
    "        inter_example_sep: str,\n",
    "        engine: str = None,\n",
    "        temperature: float = None,\n",
    "    ) -> None:\n",
    "        self.question_prefix = question_prefix\n",
    "        self.answer_prefix = answer_prefix\n",
    "        self.intra_example_sep = intra_example_sep\n",
    "        self.inter_example_sep = inter_example_sep\n",
    "        self.engine = engine\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def make_query(self, prompt: str, question: str) -> str:\n",
    "        return (\n",
    "            f\"{prompt}{self.question_prefix}{question}{self.intra_example_sep}{self.answer_prefix}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def retry_parse_fail_prone_cmd(\n",
    "    func,\n",
    "    max_retries: int = 3,\n",
    "    exceptions=(\n",
    "        ValueError,\n",
    "        KeyError,\n",
    "        IndexError,\n",
    "    ),\n",
    "):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        retries = max_retries\n",
    "        while retries:\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            except exceptions as e:\n",
    "                stack_trace = traceback.format_exc()\n",
    "\n",
    "                retries -= 1\n",
    "                print(f\"An error occurred: {e}. {stack_trace}. Left retries: {retries}.\")\n",
    "        return None\n",
    "\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of feedback_no_update.py\n",
    "import pandas as pd\n",
    "# from prompt_lib.backends import openai_api\n",
    "\n",
    "# from src.utils import Prompt\n",
    "\n",
    "\n",
    "class GSMFeedback(Prompt):\n",
    "    def __init__(self, engine: str, prompt_examples: str, temperature: float, max_tokens: int = 300) -> None:\n",
    "        super().__init__(\n",
    "            question_prefix=\"\",\n",
    "            answer_prefix=\"\",\n",
    "            intra_example_sep=\"\\n\\n\",\n",
    "            inter_example_sep=\"\\n\\n### END ###n\\n\",\n",
    "            engine = engine,\n",
    "            temperature = temperature\n",
    "        )\n",
    "        self.max_tokens = max_tokens\n",
    "        self.instruction = \"# There is an error in the code above because of lack of understanding of the question. What is the error? To find the error, go through semantically complete blocks of the code, and check if everything looks good.\"\n",
    "        self.setup_prompt_from_examples_file(prompt_examples)\n",
    "\n",
    "    def setup_prompt_from_examples_file(self, examples_path: str) -> str:\n",
    "        with open(examples_path, \"r\") as f:\n",
    "            self.prompt = f.read()\n",
    "    \n",
    "    def __call__(self, solution: str):\n",
    "        generation_query = self.make_query(solution=solution)\n",
    "        entire_output = generate_text(generation_query, model, tokenizer)\n",
    "        \n",
    "        print(entire_output)\n",
    "        \n",
    "        if \"### END\" in entire_output:\n",
    "            entire_output = entire_output.split(\"### END\")[0]\n",
    "        solution = entire_output.split(\"def solution():\")[1]\n",
    "        feedback = entire_output.split(\"def solution():\")[0]\n",
    "        solution = \"def solution():\" + solution.rstrip()\n",
    "        return {\"solution\": solution, \"feedback\": feedback}\n",
    "\n",
    "    def make_query(self, solution: str):\n",
    "        solution = f\"\"\"{self.question_prefix}{solution}{self.intra_example_sep}{self.instruction}{self.answer_prefix}\"\"\"\n",
    "        return f\"{self.prompt}{solution}\"\n",
    "    \n",
    "\n",
    "def test():\n",
    "    task_fb = GSMFeedback(\n",
    "        prompt_examples=\"../../data/prompt/gsm/feedback.txt\",\n",
    "        engine=\"code-davinci-002\",\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    wrong_soln = \"\"\"def solution():\n",
    "    \\\"\\\"\\\"Twenty dozen cups cost $1200 less than the total cost of half a dozen plates sold at $6000 each. Calculate the total cost of buying each cup.\\\"\\\"\\\"\n",
    "    plates = 6\n",
    "    plate_cost = 6000\n",
    "    cups = 12 * 20\n",
    "    cup_cost = (plates * plate_cost) / cups - 1200\n",
    "    result = cup_cost\n",
    "    return result\"\"\"\n",
    "    feedback_and_solution = task_fb(wrong_soln)\n",
    "    print(feedback_and_solution[\"feedback\"])\n",
    "    print(feedback_and_solution[\"solution\"])\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from src.utils import Prompt\n",
    "\n",
    "# from prompt_lib.backends import openai_api\n",
    "\n",
    "\n",
    "class GSMInit(Prompt):\n",
    "    def __init__(self, prompt_examples: str, engine: str, temperature: float) -> None:\n",
    "        super().__init__(\n",
    "            question_prefix=\"# Q: \",\n",
    "            answer_prefix=\"# solution using Python:\\n\",\n",
    "            intra_example_sep=\"\\n\",\n",
    "            inter_example_sep=\"\\n\\n\",\n",
    "            engine=engine,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        self.setup_prompt_from_examples_file(prompt_examples)\n",
    "\n",
    "    def setup_prompt_from_examples_file(self, prompt_examples) -> str:\n",
    "        with open(prompt_examples, \"r\") as f:\n",
    "            self.prompt = f.read()\n",
    "    \n",
    "    def make_query(self, solution: str) -> str:\n",
    "        solution = solution.strip()\n",
    "        query = f\"{self.prompt}{self.question_prefix}{solution}{self.intra_example_sep}{self.answer_prefix}\"\n",
    "        return query\n",
    "\n",
    "    def __call__(self, solution: str) -> str:\n",
    "        generation_query = self.make_query(solution)\n",
    "        output = generate_text(generation_query, model, tokenizer)\n",
    "\n",
    "        # solution_code = openai_api.OpenaiAPIWrapper.get_first_response(output)\n",
    "        solution_code = output\n",
    "\n",
    "        return solution_code.strip()\n",
    "\n",
    "\n",
    "def test():\n",
    "    task_init = GSMInit(\n",
    "        prompt_examples=\"../../data/prompt/gsm/init.txt\",\n",
    "        engine=\"code-davinci-002\",\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    question = \"The educational shop is selling notebooks for $1.50 each and a ballpen at $0.5 each.  William bought five notebooks and a ballpen. How much did he spend in all?\"\n",
    "    print(task_init(question))\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of run.py\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "#Append to absolute path\n",
    "import sys\n",
    "sys.path.append(\"/home/raj/code/nlp_project/self-refine\")\n",
    "# from src.gsm.task_init import GSMInit\n",
    "# from src.gsm.feedback import GSMFeedback\n",
    "\n",
    "from src.utils import retry_parse_fail_prone_cmd\n",
    "\n",
    "CODEX = \"code-davinci-002\"\n",
    "# GPT3 = \"text-davinci-003\"\n",
    "ENGINE = CODEX\n",
    "\n",
    "\n",
    "@retry_parse_fail_prone_cmd\n",
    "def iterative_gsm(question: str, max_attempts: int, feedback_type: str, temperature: float):\n",
    "\n",
    "    # initialize all the required components\n",
    "\n",
    "    # generation of the first fast version\n",
    "    task_init = GSMInit(engine=ENGINE, prompt_examples=\"../../data/prompt/gsm/init.txt\", temperature=temperature)\n",
    "\n",
    "    # getting feedback\n",
    "    if feedback_type == \"naive\":\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        task_feedback = GSMFeedback(engine=ENGINE, prompt_examples=\"../../data/prompt/gsm/feedback.txt\", temperature=0.7)\n",
    "\n",
    "\n",
    "    n_attempts = 0\n",
    "\n",
    "    log = []\n",
    "\n",
    "    while n_attempts < max_attempts:\n",
    "\n",
    "        if n_attempts == 0:\n",
    "            solution = task_init(solution=question)\n",
    "\n",
    "        fb_and_maybe_soln = task_feedback(solution=solution)\n",
    "        \n",
    "\n",
    "        log.append({\"attempt\": n_attempts, \"solution_curr\": solution, \"solution_fixed\": fb_and_maybe_soln[\"solution\"], \"feedback\": fb_and_maybe_soln[\"feedback\"]})\n",
    "\n",
    "        if \"it is correct\" in fb_and_maybe_soln[\"feedback\"].lower():\n",
    "            break\n",
    "\n",
    "        solution = fb_and_maybe_soln[\"solution\"]\n",
    "\n",
    "        n_attempts += 1\n",
    "\n",
    "    return log\n",
    "\n",
    "\n",
    "def fix_gsm(gsm_task_file: str, max_attempts: int, outfile: str, feedback_type: str, temperature: float):\n",
    "\n",
    "\n",
    "    slow_programs_df = pd.read_json(gsm_task_file, lines=True, orient=\"records\")\n",
    "    slow_programs_df[\"run_logs\"] = None\n",
    "    results = []\n",
    "    for i, row in tqdm(slow_programs_df.iterrows(), total=len(slow_programs_df)):\n",
    "        row_copy = row.to_dict()\n",
    "        try:\n",
    "            run_logs = iterative_gsm(question=row[\"input\"], max_attempts=max_attempts, feedback_type=feedback_type, temperature=temperature)\n",
    "            row_copy[\"run_logs\"] = run_logs\n",
    "            row_copy[\"generated_answer_ours\"] = run_logs[-1][\"solution_fixed\"]\n",
    "            row_copy[\"generated_answer_direct\"] = run_logs[0][\"solution_curr\"]\n",
    "            results.append(row_copy)\n",
    "            if i % 10 == 0:\n",
    "                pd.DataFrame(results).to_json(outfile + f\".{i}.jsonl\", orient=\"records\", lines=True)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "            # pass\n",
    "    pd.DataFrame(results).to_json(outfile, orient=\"records\", lines=True)\n",
    "    return results\n",
    "\n",
    "\n",
    "def test():\n",
    "    import json\n",
    "\n",
    "    \n",
    "    with open(\"/tmp/debug_gsm.jsonl\", \"w\") as fout:\n",
    "        fout.write(json.dumps({\"input\": \"Twenty dozen cups cost $1200 less than the total cost of half a dozen plates sold at $6000 each. Calculate the total cost of buying each cup.\"}))\n",
    "        \n",
    "    logs = fix_gsm(\n",
    "        gsm_task_file=\"/tmp/debug_gsm.jsonl\", max_attempts=3, outfile=\"/tmp/test.jsonl\", feedback_type=\"rich\", temperature=0.0\n",
    "    )\n",
    "    for i, log in enumerate(logs):\n",
    "        print(log[\"generated_answer_ours\"])\n",
    "        print(log[\"generated_answer_direct\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch, tqdm\n",
    "from transformers import LlamaModel, LlamaTokenizer, AutoModelForMaskedLM, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def load_model(model_name, token):\n",
    "    # Load model and tokenizer with authentication token\n",
    "    # model = LlamaModel.from_pretrained(model_name, use_auth_token=token)\n",
    "    # tokenizer = LlamaTokenizer.from_pretrained(model_name, use_auth_token=token)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token, cache_dir=\"/home/raj/models/\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=token, cache_dir=\"/home/raj/models/\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_model_from_local(model_file_path):\n",
    "    # Load model and tokenizer from local cache\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_file_path, local_files_only=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_file_path, local_files_only=True)\n",
    "    model = None\n",
    "    return model, tokenizer\n",
    "\n",
    "# Specify the model name and your Hugging Face access token\n",
    "# model_name_gsm = \"reciprocate/llama2-7b-gsm8k\"\n",
    "model_name_llama = \"meta-llama/Llama-2-7b-hf\"\n",
    "your_token = \"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\"  # Replace with your actual token\n",
    "# model_file_path = \"/home/raj/.cache/huggingface/hub/models--reciprocate--llama2-7b-gsm8k/snapshots/a99b9c5a7e7b6c37dc6fd81cbc4fd2f2015b2967\"\n",
    "# model_gsm, tokenizer_gsm = load_model(model_name, your_token)\n",
    "\n",
    "model = None\n",
    "model_llama = None\n",
    "model_gsm = None\n",
    "tokenizer = None\n",
    "tokenizer_llama = None\n",
    "tokenizer_gsm = None\n",
    "\n",
    "gc.collect()\n",
    "model_llama, tokenizer_llama = load_model(model_name_llama, your_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"gsm8k\", ignore_verifications=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(dataset))\n",
    "(len(dataset))\n",
    "dataset['train']\n",
    "dataset['test']\n",
    "\n",
    "# Load the data instances\n",
    "train_data = dataset['train']\n",
    "test_data = dataset['test']\n",
    "\n",
    "# Iterate over the first few instances\n",
    "for i, instance in enumerate(train_data):\n",
    "    print(f\"Instance {i + 1}:\", instance)\n",
    "    if i == 5:\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_llama.to('cpu')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "def generate_text(prompt, model, tokenizer):\n",
    "    # Encode the prompt to tokens\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    # Generate tokens response from the model\n",
    "    output_ids = model.generate(input_ids, max_length=200, num_return_sequences=2)\n",
    "    \n",
    "    # print(output_ids.shape)    \n",
    "    # print(output_ids)\n",
    "    \n",
    "    # Decode the generated ids to a text string\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"what is the value of 3 + 5 ?\"\n",
    "\n",
    "# Generate text based on the prompt\n",
    "generated_text = generate_text(prompt, model_llama, tokenizer_llama)\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompt:\n",
    "    def __init__(\n",
    "        self,\n",
    "        question_prefix: str,\n",
    "        answer_prefix: str,\n",
    "    ) -> None:\n",
    "        self.question_prefix = question_prefix\n",
    "        self.answer_prefix = answer_prefix\n",
    "\n",
    "    def make_query(self, prompt: str, question: str) -> str:\n",
    "        return (\n",
    "            f\"{prompt}{self.question_prefix}{question}{self.answer_prefix}\"\n",
    "        )\n",
    "\n",
    "class GSMInit(Prompt):\n",
    "    def __init__(self, prompt_examples: str = None, model=None, tokenizer=None) -> None:\n",
    "        super().__init__(\n",
    "            question_prefix=\"# Question: \\t\",\n",
    "            answer_prefix=\"# Answer: \\t\",\n",
    "        )\n",
    "        # self.setup_prompt_from_examples_file(prompt_examples)\n",
    "        self.prompt = \"Solve the following math question. Keep your answer short and concise.\\n\\n\"\n",
    "        self.one_shot = \"For example, \\n # Q: \\t'What is 3 + 5 ?'\\n # Answer: 8.\\n\"\n",
    "        self.prompt += self.one_shot\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def setup_prompt_from_examples_file(self, prompt_examples) -> str:\n",
    "        with open(prompt_examples, \"r\") as f:\n",
    "            self.prompt = f.read()\n",
    "    \n",
    "    def make_query(self, question: str) -> str:\n",
    "        question = question.strip()\n",
    "        query = f\"{self.prompt}{self.question_prefix}{question}{self.answer_prefix}\"\n",
    "        return query\n",
    "\n",
    "    def __call__(self, question: str) -> str:\n",
    "        generation_query = self.make_query(question)\n",
    "        output = generate_text(generation_query, self.model, self.tokenizer)\n",
    "        return output.strip()\n",
    "\n",
    "task_init = GSMInit(model = model_llama, tokenizer = tokenizer_llama)\n",
    "\n",
    "for i, instance in enumerate(train_data):\n",
    "    # print(instance)\n",
    "    question = instance['question']\n",
    "    answer = instance['answer']\n",
    "    answer = answer.split(\"####\")[1].strip()\n",
    "    try:\n",
    "        answer = float(answer)\n",
    "    except:\n",
    "        pass\n",
    "    model_answer = task_init(question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(f\"Model Answer: {model_answer}\")\n",
    "\n",
    "    if i >=5 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, instance in enumerate(test_data):\n",
    "    # print(instance)\n",
    "    \n",
    "    question = instance['question']\n",
    "    answer = instance['answer']\n",
    "    \n",
    "    print(instance)\n",
    "    \n",
    "    if i >= 10:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
